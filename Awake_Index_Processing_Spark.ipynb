{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\"><h1>AWAKE Data Reduction</h1></div>\n",
    "<div style=\"text-align:center\"><h2><i>Large Scale Data Reduction of AWAKE experiment data (HDF5 files) with Apache Spark</i></h2></div>\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates accessing the AWAKE CSV compressed database and visualizing the extracted datasets  \n",
    "***Author: Aman Singh Thakur***  \n",
    "***Contact: Prasanth Kothuri***  \n",
    "  \n",
    "To run this notebook we used the following configuration:\n",
    "* *Software stack*: LCG_96 Python3\n",
    "* *Platform*: centos8-gcct\n",
    "* *Spark Cluster*: cloud containers\n",
    "\n",
    "This is developed in the context of CERN-HSF Google Summer Of Code \n",
    "<img align=\"left\" src=\"https://developers.google.com/open-source/gsoc/resources/downloads/GSoC-logo-horizontal-200.png\" alt=\"Italian Trulli\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n",
      "215769\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py, io\n",
    "import shutil\n",
    "import csv\n",
    "from csv import reader\n",
    "from pyspark.sql.readwriter import DataFrameWriter\n",
    "from pyspark.sql.functions import *\n",
    "import re\n",
    "\n",
    "print (h5py.__version__)\n",
    "\n",
    "prefix_domain_user = \"root://eosuser.cern.ch/\"\n",
    "prefix_domain = \"root://eospublic.cern.ch/\"\n",
    "dir_csv_home = \"/eos/user/p/pkothuri/CSVFiles/SparkFinal/\"\n",
    "dir_spark_files = \"/eos/user/p/pkothuri/AWAKE/CSVFiles/SparkFinal/\"\n",
    "dir_home = \"/eos/experiment/awake/event_data/\"\n",
    "dir_h5_filepath = \"/eos/experiment/awake/event_data/2016/\"\n",
    "final_pair = {}\n",
    "\n",
    "def get_all_files(dir_filepath, file_type):\n",
    "    all_files = []\n",
    "    #print(dir_filepath)\n",
    "    for file in os.listdir(dir_filepath):\n",
    "        file_fields = file.split(\".\")\n",
    "        if len(file_fields) > 1:\n",
    "            if str(file_fields[1]) == str(file_type):\n",
    "                all_files.append(prefix_domain+dir_filepath+str(file))\n",
    "                #all_files.append(dir_filepath+str(file))\n",
    "    return all_files\n",
    "\n",
    "def get_all_files_home_util(dir_h5_filepath, depth):\n",
    "    if depth == 0:\n",
    "        return get_all_files(dir_h5_filepath, \"h5\")\n",
    "    else:\n",
    "        temp_arr = os.listdir(dir_h5_filepath)\n",
    "        all_H5_files = []\n",
    "        for val in temp_arr:\n",
    "            all_H5_files+=get_all_files_home_util(dir_h5_filepath+val+\"/\", depth-1)\n",
    "    return all_H5_files\n",
    "\n",
    "def get_all_files_home(dir_h5_filepath, depth):\n",
    "    year_directories = os.listdir(dir_h5_filepath)\n",
    "    all_H5_files = []\n",
    "    all_H5_files = get_all_files_home_util(dir_h5_filepath, depth)\n",
    "    return all_H5_files\n",
    "\n",
    "def mkdir_structure(filename):\n",
    "    if not os.path.exists(dir_csv_home):\n",
    "        os.mkdir(dir_csv_home)\n",
    "    filename = filename[int(len(prefix_domain+dir_home)-1):]\n",
    "    filename_arr = filename.split(\".\")\n",
    "    file_arr = filename.split(\"/\")\n",
    "    result = dir_csv_home\n",
    "    for i in range(0, len(file_arr)-1):\n",
    "        result+=file_arr[i]+\"/\"\n",
    "        if not os.path.exists(result):\n",
    "            os.mkdir(result)\n",
    "        if i == 0:\n",
    "            year = file_arr[i]\n",
    "        elif i == 1:\n",
    "            month = file_arr[i]\n",
    "        elif i==2:\n",
    "            day = file_arr[i]\n",
    "    return year, month, day, filename_arr[0]\n",
    "\n",
    "def read_input(filepath):\n",
    "    all_h5_files = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        files_list = file.read().split(\",\")\n",
    "        for file in files_list:\n",
    "            if(file!=\"\"):\n",
    "                all_h5_files.append(file)\n",
    "    return all_h5_files\n",
    "\n",
    "def write_input(all_h5_files, filename):\n",
    "    if not os.path.exists(dir_csv_home):\n",
    "        os.mkdir(dir_csv_home)\n",
    "    with open(dir_csv_home+filename, 'w') as file:\n",
    "        for row in all_h5_files:\n",
    "            file.write(row+\",\") \n",
    "\n",
    "# check the number of files in the input            \n",
    "all_h5_files = get_all_files_home(dir_h5_filepath, 2)\n",
    "print(len(all_h5_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_database(dir_filepath):\n",
    "    shutil.rmtree(dir_filepath)\n",
    "\n",
    "#clean_database(dir_csv_home)\n",
    "# GETTING VALUE AS PER DATATYPE\n",
    "def get_dataset_value(size, file):\n",
    "    if(int(size) == 0 or int(size) > 1):\n",
    "        return \"\"\n",
    "    else:\n",
    "        if(str(file.dtype) == \"|S1\"):\n",
    "            return ord(file[0])\n",
    "        elif \"S\" in str(file.dtype):\n",
    "            return str(file[0].decode('utf-8'))\n",
    "        else:\n",
    "            return str(file[0])    \n",
    "\n",
    "def decode_attr(file):\n",
    "    try:\n",
    "        return file.decode('utf-8')\n",
    "    except:\n",
    "        return str(file)\n",
    "    \n",
    "def get_dataset_attr(file):\n",
    "    comment = \"\"\n",
    "    acqStamp = \"nan\"\n",
    "    exception = \"False\"\n",
    "    for key in file.keys():\n",
    "        if \"comment\" in key:\n",
    "            comment = decode_attr(file[key])\n",
    "        if \"acqStamp\" in key:\n",
    "            acqStamp = str(file[key])\n",
    "        if \"exception\" in key:\n",
    "            exception = str(file[key])\n",
    "    return [comment, acqStamp, exception]\n",
    "\n",
    "def write_into_CSV(file, values, comment_hash, dataset_hash, group_attrs):\n",
    "    if(isinstance(file, h5py.Group)):\n",
    "        for sub in file.keys():\n",
    "            try:\n",
    "                group_attrs = get_dataset_attr(file.attrs)\n",
    "                #print(file.name+\" Group \"+str(group_attrs))\n",
    "            except:\n",
    "                group_attrs = [\"\", \"nan\", \"False\"]\n",
    "            if(isinstance(file[sub], h5py.Dataset)):\n",
    "                try:\n",
    "                    size = str(file[sub].size)\n",
    "                except:\n",
    "                    size = \"0\"\n",
    "                try:\n",
    "                    dataset_value = str(get_dataset_value(size, file[sub])).strip()\n",
    "                except:\n",
    "                    dataset_value = \"\"\n",
    "                try:\n",
    "                    datatype = str(file[sub].dtype).strip()\n",
    "                except:\n",
    "                    datatype = \"\"\n",
    "                if (group_attrs[0]==\"\" and group_attrs[1]==\"nan\" and group_attrs[2]==\"False\"):\n",
    "                    group_attrs = get_dataset_attr(file[sub].attrs)\n",
    "                #print(file[sub].name+\" \"+str(group_attrs))\n",
    "                dataset_hash.add(file[sub].name)\n",
    "                if group_attrs[0] != \"\":\n",
    "                    comment_hash[group_attrs[0]] = file[sub].name\n",
    "                values.append([file[sub].name, file.name, group_attrs[1], group_attrs[2],\n",
    "                                      dataset_value, \"\\\"\"+str(file[sub].shape)+\"\\\"\", str(datatype)])\n",
    "            elif (isinstance(file[sub], h5py.Group)):\n",
    "                    write_into_CSV(file[sub], values, comment_hash, dataset_hash, group_attrs)\n",
    "    return values, comment_hash, dataset_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractHDF5(hdf5file):\n",
    "    prefix = hdf5file[0]\n",
    "    content = hdf5file[1]\n",
    "    f=h5py.File(io.BytesIO(content))\n",
    "    #f = h5py.File(hdf5file)\n",
    "    central_csv_db = [] # It'll be loader later\n",
    "    comment_hash = {}\n",
    "    group_attrs = [\"\", \"nan\", \"False\"]\n",
    "    dataset_hash = set()\n",
    "    values, comment_hash, dataset_hash = write_into_CSV(f, [], comment_hash, dataset_hash, group_attrs)\n",
    "    result = \"DatasetName,GroupName,AcqStamp,Exception,DatasetValue,Shape,Datatype\\n\"   \n",
    "    for row in values :\n",
    "        for i in range(0, len(row)):\n",
    "            result+=str(row[i])\n",
    "            if i != len(row)-1:\n",
    "                result+=\",\"\n",
    "        result+=\"\\n\"\n",
    "    comment_result = \"\"\n",
    "    for key in comment_hash.keys():\n",
    "        comment_result += \"\\\"\"+key+\"\\\"\"+\",\"+comment_hash[key]+\"\\n\"\n",
    "    dataset_result = \"\"\n",
    "    for val in dataset_hash:\n",
    "        dataset_result+=val+\",\"\n",
    "    #print(comment_result, dataset_result)\n",
    "    final_pair[prefix]=result\n",
    "    final_pair[\"comment_\"+prefix]=comment_result\n",
    "    final_pair[\"dataset_\"+prefix]=dataset_result\n",
    "    return final_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData = sc.binaryFiles(','.join(all_h5_files))\n",
    "hdf5_reduced_collection = inputData.map(lambda x: extractHDF5(x))\n",
    "if not os.path.exists(dir_csv_home):\n",
    "    os.mkdir(dir_csv_home)\n",
    "\n",
    "if os.path.exists(dir_spark_files):\n",
    "    clean_database(dir_spark_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    hdf5_reduced_collection.repartition(10000).saveAsTextFile(prefix_domain_user+dir_spark_files)\n",
    "except Exception as e:\n",
    "     # There is a bug in xrootd-connector and we can ignore it\n",
    "    if \"ch.cern.eos.XRootDFileSystem.delete\" in str(e):\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_csv_home+\"Central/\"+\"dataset_hash.csv\", 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for val in dataset_hash:\n",
    "        writer.writerow([val])\n",
    "    csvfile.close()\n",
    "\n",
    "with open(dir_csv_home+\"Central/\"+\"comment_hash.csv\", 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for key in comment_hash.keys():\n",
    "        #print([key, comment_hash[key]])\n",
    "        writer.writerow([key, comment_hash[key]])\n",
    "    csvfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "sparkconnect": {
   "bundled_options": [
    "SparkMetrics",
    "RDataFrame",
    "LongRunningAnalysis"
   ],
   "list_of_options": [
    {
     "name": "spark.executor.memory",
     "value": "4g"
    },
    {
     "name": "spark.kubernetes.memoryOverheadFactor",
     "value": "0.4"
    },
    {
     "name": "spark.dynamicAllocation.maxExecutors",
     "value": "64"
    },
    {
     "name": "spark.executor.cores",
     "value": "4"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
